{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glFXKMBlEY5-",
    "outputId": "b9c14850-73d6-419b-de95-302ab3884d33"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-c9jz4JEhrE",
    "outputId": "ce7126d4-c7c2-491b-b2fa-e1362edbc86f"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U datasets peft trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8GYiAT5EqfD"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Q&A Fine-Tuning Dataset Generation with Batch Processing and Checkpointing\n",
    "Processes all text chunks efficiently on free Colab GPU\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493,
     "referenced_widgets": [
      "82b83d39c9fa46fd940b504e55fc5aa1",
      "23c77ab033124f8a908ec4e25e08027e",
      "badd5269041f49b79978a8dc704dec6d",
      "5bb51d290624439eac7cb79e4a26e634",
      "80eed19ca0564fc68ea1914eb0480f9a",
      "ba55b67e6b9f4b29b503301f90f35aa3",
      "6f2c40744ca64106a4d4fb6b15876d6f",
      "50c1f6eb8ffa42adb4ba2a3af0ea1920",
      "7cc4f4f1c24b49f7ac8a1c514fa5d422",
      "01441ead21d241809c0bd83319a67267",
      "3a5febd2dee34dbd93a1fc4804ecdd9c",
      "1e86b5d0748344608d14504e69f37597",
      "61a8c3ed2e4c4d31a13628af070ed455",
      "95677dbc24094eaf98e7d1bedfffd8a4",
      "04aab68218ff4d9283a830ca9aa1ae2e",
      "21fc2f68afa04e5587a708335f0b11fd",
      "77e4249a28f34f698da365c19783b288",
      "b05454544ac0497b9ab0deb7a2f84539",
      "d2848bf35421437cb2142a92b95531a2",
      "f607f0eef4a640ecbd08856cc6154641",
      "2095a6f9228647ceade5c3f99d850db5",
      "34516d62de6e4bb2a1028a6b52fe31e6",
      "c8ed9111aecb46568d449d26ba0d613c",
      "9c370aa055be4f3bb9502d137e57a724",
      "dd0d44bfd9314fa98f1cdadcb7cf57b8",
      "e20c073f28df4e72a52826ddc341a747",
      "a676c01282514cab8156144440a753bc",
      "9561dad1a91b41f085b0b3a161e79052",
      "686f7da14f214a6592175ceaa75570f9",
      "36e546e94ae640acbb7e346c6001dc48",
      "30b80ef99be545f18af8f6383f1a6940",
      "35f35204546e430f8c7e9ba46c4dcf26",
      "aa04b957087e4e88b4e79d5de09f82e3",
      "87146907d4b543e881ec680974869b01",
      "26427bf47c0544b38612aa60f97195aa",
      "e2b5b4702a7440ceb2444a8d92e0f0c4",
      "1f55c69a1608476aa20424341176ef62",
      "850ab682ee76481c96c5ffe595d338ec",
      "ea6eaf924bd04fbf86652c28ca6c1568",
      "fed8d4aebeff4ff4a2d77e6c4606758c",
      "c689ba1162c84520a5a8ecf579ec5198",
      "cda0a0792a1e4983959833663ca33697",
      "95c4cbe80d1b4a4a8126d383390f9cf8",
      "2eb50de68b364011b80f34f6de6a7322",
      "47199791f07f4be5839967ea817d2fec",
      "b076b1301c804031a646bd7ca9f46716",
      "0517ea7e99dd47a18a3f0829c66b3c33",
      "175f7603df57456282766d40429b8193",
      "412a848663c24cf2add6924cf141f776",
      "179000ce9b944bd49fb47e7cfed56479",
      "30e5fc047d394cbb8302471a8222e8db",
      "b9000426d71e478d84de440bd8e42b4e",
      "2696474c31664ddb84ed4ed95d78ac83",
      "431135a6274b4802aba1041b9115a390",
      "280993cea0434c37a297d246a9f6357e",
      "9994e042adbf4dd1bb7761c9824a95c9",
      "4b77775233ad45d095f31d87beb3fe54",
      "e6560bc6cab94d649d0aaaa1317279cd",
      "3b86c9ca65974a719dc26d2f568aed08",
      "274b581702cb40c2b5e66b1ad646d84e",
      "0400561987624890a4ffd80ecde69e7d",
      "3d25bd279b254c4face8550e64b64d20",
      "27284abd0c9d4c4d82e62b9d2ce2acdf",
      "5ecdfc8021c14fa687b60431099817fe",
      "54246d6ead03403780a6592e3ffee5b7",
      "a65d8f4a0207453190731a10aa313a42",
      "d580a0f359074fcdae582cdb54f8e324",
      "e88928133e334e618b5a3ad940cba4a5",
      "552d60d6e7244021a61dfb5e4f6a1ba8",
      "712b97418fc64d5faa1d0f7c5e3aca1d",
      "9f97b4292b244978aa239d9eadb949b2",
      "5dc3ce1eb7594f0e9b8b19d023c5ebd9",
      "6fab7276aae74922addb7ff4ce7ec7b2",
      "184bc15c8d4a4949924b6badf3e156c3",
      "199d110367ca45b3b45c5a7d864bdf1b",
      "52956fa4304c42c08690d9ff0122c08e",
      "3068140991694825872e90f24409b8b3",
      "27fcd11ab87d4659b5f92f20bd0ed179",
      "b7f4786836604f828e6a94252e3774e5",
      "05c768254c5046128a9fa4fa67cfee60",
      "c0a6d2c2fff446bf952c085830a3c21f",
      "5d0f38fa3042412b9171031dd95a1039",
      "f79ed980b7414073a83bea2cfb5fca2b",
      "481ef400de494af6a7a924f2622fc119",
      "9cea7e60684341c7a8405180bcc225da",
      "911ac45fcd6c41dfb977f386ebc7666b",
      "e2ad0b70be3b49f59594883356f1c4ba",
      "af6dcc416392404fa9e038c847b8993b",
      "dbb4784d87ae47e2a31c6741438ce643",
      "37b531665c944c6282617e4641b38bc9",
      "1f06c352c99d486582163c171437bce7",
      "4e25c6bb5d004fc1b6af7cfeda34d43a",
      "5b1b740332dd4d44941cf84b17538044",
      "8f94e6317bf6497880b48b04b4bfe7b7",
      "07c3a17c7912415ca3904f5b91f8108d",
      "bb0cc2b4535b4ed197438d4d61365289",
      "32f6024de06f44cd8f9e8b3d875210e3",
      "ba6d0b8087784f69ac6e3e914a0f9c9a",
      "2f32cba1991a449ba3a5105f2e15d090",
      "87d3af4a28c441a1af8b2b90e49ed494",
      "006776765cdb4e4b82f8978f1f7b1764",
      "ac19c94b915f4069b1d0318435815031",
      "3a8bc50f0a734536bf2b142a073c8a0a",
      "e666f850812b4c0cb002c29aef4d16eb",
      "3357dcf7326148d380ff0b56fef560ab",
      "df44f4e96701443fbecd61d0eaf46052",
      "83790709354440ba9b28f31262a28b55",
      "57ba2bd47843444abbb3e27a32d64992",
      "01945cb6a9394e47b34edc5d6d54548c",
      "fc31faa8dd944ad69fcd0d1ba98a2e23",
      "5d6e5b0d22f445488a4297c3af07f378",
      "94be974d81de4281a151b93d83f2014b",
      "dc5b5513fb244984b08062780ba7c6f0",
      "509cb35111d64076a4c6511f0be0d65a",
      "6a34933219ff4054af8ed7f25d3ab4a2",
      "e46301a1d774493181741e5ceaa89293",
      "434f631175104858bda67e1fa5c28132",
      "07be47372f49457f933c4bff0c95ac2b",
      "ab96c8361ffa4d34830ebe95eb86d851",
      "ec92eb5646c54ed59270cd48c7d45ae0",
      "966f43ad59f946c8889de241d9796d1a"
     ]
    },
    "id": "clLm8neOEs71",
    "outputId": "c5f86572-4f88-4b05-a2a5-b7e8d1ac4fbe"
   },
   "outputs": [],
   "source": [
    "# SETUP: Authentication and Model Configuration\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "except ImportError:\n",
    "    hf_token = getpass(\"Enter your Hugging Face Access Token: \")\n",
    "\n",
    "login(token=hf_token)\n",
    "\n",
    "MODEL_ID = \"google/gemma-2b-it\"\n",
    "CLEAN_TEXT_PATH = \"/content/master_dataset_v4.txt\"\n",
    "OUTPUT_DATASET_PATH = \"/content/qa_finetuning_dataset.jsonl\"\n",
    "CHECKPOINT_FILE = \"/content/generation_checkpoint.txt\"\n",
    "\n",
    "# Configuration for batch processing\n",
    "BATCH_SIZE = 100 \n",
    "CHUNK_SIZE = 1500\n",
    "CHUNK_OVERLAP = 150\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dNKQjnwFFdQ",
    "outputId": "4264fb54-f99b-43fa-ecdb-c01686d70b4f"
   },
   "outputs": [],
   "source": [
    "# TEXT PROCESSING: Load and Chunk Text\n",
    "\n",
    "def create_text_chunks(text, chunk_size=1500, overlap=150):\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "print(\"Loading and chunking text...\")\n",
    "with open(CLEAN_TEXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    clean_text = f.read()\n",
    "\n",
    "all_text_chunks = create_text_chunks(clean_text)\n",
    "print(f\"Total chunks available: {len(all_text_chunks)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiFSBkwDFH0u"
   },
   "outputs": [],
   "source": [
    "# QA GENERATION: Prompt and Generation Function\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"<s>[INST]\n",
    "You are an expert in creating educational data. Your task is to read the provided text from a computer science textbook and generate three high-quality question-answer pairs.\n",
    "\n",
    "Your goal is to emulate the style of the Stanford Question Answering Dataset (SQuAD). This means:\n",
    "1.  **Strictly Grounded:** The answer to every question must be a direct quote or a very close paraphrase of a sentence found within the provided text.\n",
    "2.  **Natural Questions:** The questions should be phrased as a student would naturally ask them.\n",
    "3.  **Concise Answers:** The answers should be as short as possible while still being comprehensive.\n",
    "\n",
    "Your final output must be ONLY a valid JSON list `[...]` containing three JSON objects `{{...}}`, where each object has a \"question\" key and an \"answer\" key. Do not add any other text, explanations, or markdown.\n",
    "\n",
    "**Textbook Chunk:**\n",
    "---\n",
    "{text_chunk}\n",
    "---\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "def generate_qa_pairs(text_chunk):\n",
    "    \"\"\"Generate Q&A pairs for a given text chunk\"\"\"\n",
    "    prompt = PROMPT_TEMPLATE.format(text_chunk=text_chunk)\n",
    "    try:\n",
    "        response = hf_pipeline(prompt)\n",
    "        response_text = response[0]['generated_text'].split('[/INST]')[-1].strip()\n",
    "\n",
    "        # Extract JSON from response\n",
    "        start_index = response_text.find('[')\n",
    "        end_index = response_text.rfind(']')\n",
    "\n",
    "        if start_index != -1 and end_index != -1:\n",
    "            json_response = response_text[start_index:end_index + 1]\n",
    "            return json.loads(json_response)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQA6XOxWFKVB"
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT MANAGEMENT\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load the last checkpoint index\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        try:\n",
    "            with open(CHECKPOINT_FILE, 'r') as f:\n",
    "                return int(f.read().strip())\n",
    "        except:\n",
    "            return 0\n",
    "    return 0\n",
    "\n",
    "def save_checkpoint(chunk_index):\n",
    "    \"\"\"Save current checkpoint\"\"\"\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        f.write(str(chunk_index))\n",
    "\n",
    "def count_generated_pairs():\n",
    "    \"\"\"Count existing Q&A pairs in output file\"\"\"\n",
    "    if not os.path.exists(OUTPUT_DATASET_PATH):\n",
    "        return 0\n",
    "    with open(OUTPUT_DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "        return len(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GgKKm9VFM_R"
   },
   "outputs": [],
   "source": [
    "# MAIN GENERATION LOOP WITH BATCH PROCESSING\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process all text chunks with batching and checkpointing\"\"\"\n",
    "\n",
    "    # Initialize or resume from checkpoint\n",
    "    start_chunk = load_checkpoint()\n",
    "    existing_pairs = count_generated_pairs()\n",
    "\n",
    "    if start_chunk > 0:\n",
    "        print(f\"Resuming from chunk {start_chunk}\")\n",
    "        print(f\"Existing pairs in output: {existing_pairs}\\n\")\n",
    "    else:\n",
    "        print(\"Starting fresh generation\\n\")\n",
    "        with open(OUTPUT_DATASET_PATH, 'w', encoding='utf-8') as f:\n",
    "            pass\n",
    "\n",
    "    total_chunks = len(all_text_chunks)\n",
    "    total_pairs_generated = 0\n",
    "\n",
    "    num_batches = (total_chunks + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "    print(f\"Total chunks to process: {total_chunks}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Total batches: {num_batches}\\n\")\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start_idx = batch_idx * BATCH_SIZE\n",
    "        batch_end_idx = min(batch_start_idx + BATCH_SIZE, total_chunks)\n",
    "\n",
    "        if batch_end_idx <= start_chunk:\n",
    "            continue\n",
    "\n",
    "        if batch_start_idx < start_chunk:\n",
    "            batch_start_idx = start_chunk\n",
    "\n",
    "        batch_chunks = all_text_chunks[batch_start_idx:batch_end_idx]\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Batch {batch_idx + 1}/{num_batches} (chunks {batch_start_idx}-{batch_end_idx-1})\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        batch_pairs_count = 0\n",
    "\n",
    "        # Generate Q&A pairs for this batch\n",
    "        for chunk_idx, chunk in enumerate(tqdm(batch_chunks, desc=\"Generating Q&A pairs\")):\n",
    "            absolute_chunk_idx = batch_start_idx + chunk_idx\n",
    "            qa_pairs = generate_qa_pairs(chunk)\n",
    "\n",
    "            if qa_pairs:\n",
    "                for pair in qa_pairs:\n",
    "                    with open(OUTPUT_DATASET_PATH, 'a', encoding='utf-8') as f:\n",
    "                        f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
    "                    batch_pairs_count += 1\n",
    "                    total_pairs_generated += 1\n",
    "\n",
    "            save_checkpoint(absolute_chunk_idx + 1)\n",
    "\n",
    "        print(f\"\\nBatch {batch_idx + 1} Summary:\")\n",
    "        print(f\"  - Chunks processed: {batch_start_idx} to {batch_end_idx-1}\")\n",
    "        print(f\"  - Q&A pairs generated in this batch: {batch_pairs_count}\")\n",
    "        print(f\"  - Total pairs generated so far: {total_pairs_generated}\")\n",
    "        print(f\"  - Output file: {OUTPUT_DATASET_PATH}\")\n",
    "\n",
    "        # Clear GPU cache between batches to prevent memory buildup\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"  - GPU cache cleared\")\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"GENERATION COMPLETE!\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    final_pair_count = count_generated_pairs()\n",
    "    print(f\"Total Q&A pairs generated: {final_pair_count}\")\n",
    "    print(f\"Dataset saved to: {OUTPUT_DATASET_PATH}\")\n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  - Total chunks processed: {total_chunks}\")\n",
    "    print(f\"  - Total Q&A pairs: {final_pair_count}\")\n",
    "    print(f\"  - Average pairs per chunk: {final_pair_count / total_chunks:.2f}\")\n",
    "\n",
    "    # Clean up checkpoint file after successful completion\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        os.remove(CHECKPOINT_FILE)\n",
    "        print(\"\\nCheckpoint file cleaned up.\")\n",
    "\n",
    "    print(\"\\nReady for fine-tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWv5zVVkRcPH",
    "outputId": "04168863-3bf5-4900-9deb-d652aed98dc8"
   },
   "outputs": [],
   "source": [
    "# SCRIPT ENTRY POINT\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nGeneration interrupted by user.\")\n",
    "        print(f\"Progress saved. Run again to resume from checkpoint.\")\n",
    "        print(f\"Current checkpoint: {load_checkpoint()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nError occurred: {str(e)}\")\n",
    "        print(f\"Progress saved. Run again to resume from checkpoint.\")\n",
    "        print(f\"Current checkpoint: {load_checkpoint()}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxSWt6Ei3nNO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "except ImportError:\n",
    "    hf_token = getpass(\"Enter your Hugging Face Access Token: \")\n",
    "\n",
    "login(token=hf_token)\n",
    "\n",
    "BASE_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "DATASET_PATH = \"/content/qa_finetuning_dataset.jsonl\"\n",
    "NEW_ADAPTERS_ID = \"algo-tutor-mistral-7b-adapters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shKggDZg34EK",
    "outputId": "87be3aad-d97f-4035-bd07-b28f67a88bd0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "INPUT_FILE = \"/content/qa_finetuning_dataset.jsonl\"\n",
    "OUTPUT_FILE = \"/content/qa_finetuning_dataset_cleaned.jsonl\"\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as fin:\n",
    "    text = fin.read()\n",
    "\n",
    "text = text.replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "json_candidates = re.findall(r'\\{.*?\\}', text, re.DOTALL)\n",
    "\n",
    "valid_count = 0\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for candidate in json_candidates:\n",
    "        try:\n",
    "            obj = json.loads(candidate)\n",
    "            q, a = obj.get(\"question\"), obj.get(\"answer\")\n",
    "\n",
    "            if not isinstance(q, str) or not q.strip():\n",
    "                continue\n",
    "\n",
    "            if isinstance(a, list):\n",
    "                a = \" \".join(map(str, a))\n",
    "            if not isinstance(a, str) or not a.strip():\n",
    "                continue\n",
    "\n",
    "            fout.write(json.dumps({\"question\": q.strip(), \"answer\": a.strip()}, ensure_ascii=False) + \"\\n\")\n",
    "            valid_count += 1\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "print(f\"Cleaned {valid_count} valid Q&A pairs.\")\n",
    "print(f\"Saved cleaned dataset to: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "4f612e53b8d14144bcb25c45d26f4f76",
      "499b46107c84434cb757e0acab912470",
      "cd1affa891004ad496510488c6c50c22",
      "8715b2f09c9e49759b2ddfe1c40c3f6e",
      "153f6c31cba74608b77645994c974438",
      "a0cfa3592a8c423db22542c803c47acc",
      "5f6d1ae715a8496f86b768a5fe57e644",
      "35b2dd1754174f579e8561e137839e73",
      "335124890b124f8f89ba558b1c5d2b00",
      "184eb94373a44b82abbb6c5ddd6e6cfb",
      "a69dd20618f042a89e9441fc503f118d"
     ]
    },
    "id": "qpIoDV7X342q",
    "outputId": "36161921-fd3d-4ffb-f7a6-c381ef9618ee"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=OUTPUT_FILE, split=\"train\")\n",
    "print(f\"Loaded dataset with {len(dataset)} examples.\")\n",
    "\n",
    "def format_instruction(sample):\n",
    "    return f\"\"\"<s>[INST] {sample['question']} [/INST]\\n{sample['answer']}</s>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3D7EPwOD3-g8",
    "outputId": "feb84296-b868-4f56-f358-3a5b851eec29"
   },
   "outputs": [],
   "source": [
    "!head qa_finetuning_dataset_cleaned.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "62c8886210414211b0654ac7c6b86939",
      "37ff116c09df4993aaf5cbd7df9118f6",
      "c1dd9ac089d142bdbb71c2ccb1d89dc7",
      "58f02bc0419c4f15aeffb48d7d2cf68b",
      "54406ebe48fc437bade0290f669aba2c",
      "4eb5ec6ef564464184fd35d2203dada2",
      "4df37874ef964e598df61e76b4de0703",
      "6b89911588fa44f39e5832eb3f7db05d",
      "442d301189ae4e0fa25d5febc7cf8c58",
      "151c457649ee463ab809ac7ff64bfd4a",
      "f4a2db74833c4951877a6bfbe9e59444",
      "a138a1c00f024465b3c3911141feb10c",
      "c80ee8a2d4bc40bf954c8d7b771e7c4c",
      "b9c026f7164c487aa060cb4f9909f071",
      "8dda1e54a7fc4a929d2628a0584f3d65",
      "36ed361c3a32460fa81ea89212970163",
      "2cc8eb0a6bcb49c983c8de49b7c8ab43",
      "5d2e90d570f84d8294e4f2eebeef0de8",
      "32a23e175dec4a6fa7cf3984840872b0",
      "9b0b1bb525334ad89e9cb563e671ce45",
      "c77d030042a946a7a90a8b005314af21",
      "647d24698a0442e387bd049007af84a3",
      "4e85349e22ad4bc2a73bafaae892f801",
      "307639defa394e86af2c0af0221ce92d",
      "6e04b1f5bf6840b0a0a214ee34fd504c",
      "0b5dee4aa6d248d89cfd97e364b8b15e",
      "9fcabd80277c4be2ac4c694fdbd1825f",
      "faf8e07c39a049dc9460bf44de137ae2",
      "89bdccbc082040e58f408fa61e790c86",
      "462e4967c3044cfe9dfc10c0472de3f6",
      "7625d93078be45bf9cfd2e8f594d0cbe",
      "6b75d93ae0084eb6a69c72a977665bdd",
      "d8a55f62de404d59907a7ed87a570204",
      "9b228d2a2e344ec2b75f22e82b8c2ca6",
      "e664d0fb23df4d458090a275af006672",
      "0069f69c9bb44bdd8d490d26d3a3eb66",
      "2a9cbe0f27984df4ba5bd57d175cfa6d",
      "e78d9147b7cf4e968f8138458b5f209e",
      "d9d9923239774ca4a4379434717a231f",
      "88e594e1705f48d28836e9db0d30d44d",
      "1c1b160290b4475392e7eca0c94ec766",
      "321e32aecea349628c1ce4c918f630a1",
      "b8d255ccbc314546adb9bbef629cd6a5",
      "8ca7ef01bfdd4cd3bc166a33d738f7e7",
      "89b325c35af344e89acbe3f6075a4dc7",
      "5c820efb9dc94d77855f668de3c75c94",
      "4c727c6afd464e1d99c4ec59271be83a",
      "7eefc399a8d847afbca2ee51c0c729d0",
      "4f1129f920b849c39975e0a3215de71d",
      "9f257b09120d4bebbd4a836be16fe32d",
      "58265828898c43989947f104779f7c99",
      "6da35e2a0fae43f68816c5da431d0399",
      "885d61a6876949ac8541ed495560ea48",
      "29bb043707ca45ad8acbd98f4a8368ef",
      "929301e1500945fcacb94a6dce8ac36f",
      "448765f73f6c47eda610f2c7feceb6aa",
      "2bca4d645ac94b848cd92b684d5cc35e",
      "0d0564cbfcdc4661b5bd623cf15fc6f7",
      "b368ab13c12744a0b7e89ef9afd4bda6",
      "1dde1c3ed91c4eba8a44f2da10929dcb",
      "d0695ae0821b4ae3b84febdbbc12a770",
      "3cfc5a0d63a74f268e5ccf2df71acd9b",
      "437ac87e510f4c41853738b92dbb9722",
      "f38d066bd00341d28684eae0c5e43c05",
      "2699d033b4754f899fbe08906d6d6b39",
      "7e9e52637ce7443fad91d95205366f0d",
      "821719f98662493bbb3440516f153187",
      "26917dad1aca4a92b8e8d9e555c6bbd7",
      "c29ce2614b1f4670b4608c61dd50ff4b",
      "39074ef09f804a9b88939dfece8d6bea",
      "6434811389b24db78a22a7aa2d301798",
      "76953a9e58544966a6b4427bdd96e1a8",
      "abb5d56bbb99425bbbbaff8496fa89f7",
      "e3a4abe52948462b8a6b8a07ddb2239d",
      "8b4b135c15b1462aaf5d9d9383f6c1a1",
      "d98e5b3caf2a420fbb57e9dccf85855c",
      "23d98a6bb21f4ae5977a73d0c280697c",
      "6d5b3b61de7041be94a3bf615167e0c5",
      "2ae53553860342f8a2c699e8fce69446",
      "7cb2fb7fcd94488193af7eb5186c7cff",
      "d2efcc302d4b489dbcb06c4185c2e6ef",
      "331d1ff43a204dd8ac4dcb3561c1a764",
      "fd72db83edc64d468c9bd2b732ea6d29",
      "23368dc3d70b4cc28b5a3935d3f9b995",
      "16fdc70288684e0e986a1a03f5485294",
      "74a183a895bc4963a7238ac86ba3c6ec",
      "3f7473be307140109d88cb0c61d7c436",
      "1c3841ef21454acaacfb4bbfc178fd98",
      "2956d6efb5054498ad388c45b71b1237",
      "b4350b529c5347f8bbe01c2da4007c3c",
      "ff685046467343e39178114e9e58bbf7",
      "cfcb434452164680abf7c7127946685d",
      "e0563cd72af94042a67db360644241b3",
      "4b0d425b56264669967eb403b69eec63",
      "7adae5dec47b46afad551639fff78da4",
      "23559449e18544d9b5aa2d9ebfa35a12",
      "c59d7a9e90e04dbfbc4a3d6ba9520161",
      "3ba4b327ddba46d1ad4fd6b7d8e1c765",
      "d0dc5647e370451d892526bb951f6de0",
      "5116a199685a4d499d5855ec027529de",
      "acdf7f6f4f284ad1b8e5e443c7ab89b1",
      "ab938b8f209249fb89f54b81e2228e19",
      "9e13dfe9af524388bac9ae76e6b890ce",
      "77b63a8edb114b34af3bfcd0d5e790bb",
      "a64fd9fbe056428e86e5969af924cf49",
      "3f4f7d10633e4ffabb319d25174b8546",
      "240c3c7a5afa4b4fb5e73bcf18bd1355",
      "432f6f29b0b44dceb653dbcc821f4238",
      "35e0672c3c4948cab1ad13b1f0b7e0ed",
      "3f56a5f4be7a4a84a5b92020bf05fdba",
      "cfc3fa5083d54995a91294a2f7633fa0",
      "27d4d3d87f7346749d0701d39b5dac3e",
      "ab75af836c70439bb008fd3faf0541bc",
      "6e0aa7eecbb14911b50294c31fef57be",
      "59907143c0b445a8ae9f7f740207f831",
      "2eaba06d4e02414288392dc8a5d843e1",
      "a2dc66335c24425999ccd1c67bf4a703",
      "5277b3f5089c412085ceac836c8b5e03",
      "9441d134732d43ce88f2797e8e53c549",
      "2f92774d32394458ae775b5db675e291",
      "e86e73854993490f90de38b27336b0e0",
      "4bb8c3c2b6894c22af4e7d28f32e16d1",
      "5118c48c68ac4a7cb5c40802e2be3e2a",
      "2e8db3dff93141ef8caba25030dd8692",
      "4f38bb6869d74d46a27660df179c4935",
      "6e13aca1c147415fa034959972888688",
      "f2be65ec58c04a72a34a5ebf33b2e1ee",
      "cf6b1948a1664c99a760cca5a85434ea",
      "d4b5a418e8454d23884c7791950b0580",
      "b73af23152d64784ae1a554b8297fc33",
      "498c46ce937847b7a5981b56d1125eb5",
      "5cf9e899026d4568a58ad230af61707c",
      "a8a105abfe4143fca1067583dacdab84",
      "cbb3b8f85b5d4700a0f4e9241d1c0933",
      "85706da5d7e143b6a32aaf0a0b81f919",
      "a2dfd47e70b14df88e9483994afd10d2",
      "81b5f56bc03242f288669c5ab413276e",
      "09ae0b51de6046c88a9f2804b50deed1",
      "14f6e212393e4113944a1f13912be906",
      "d7749421e3ee495ead209867adbc3124",
      "0d411aa1104a4610981c008a9c744feb",
      "6d0c953143094980a3de125047106935",
      "73b5ea89a0ce4d27b19049976482489e",
      "16a0f2f3794541d69498052edcf5e735",
      "67358f69e7924dbaaa3121f9631e5068",
      "0407f62cda9c42279bd8170aef6716a7",
      "fc814814149c43eb8816fff1e1e1cca3",
      "5e7dc9d115794fdeacc8c4389d295e00",
      "6dbce64f100b4f39b27e9e86e66ec32d",
      "2bc408af3e6641119fa81cc194a377cc",
      "c71910c5dd9f4d6f8113fc6b391f0930",
      "a85f6fbec5d947d398d2421df717d8e4",
      "9e0d33999b5f41cb8be37603c13e6bde",
      "257e456363a9468fa39292bbd8a2d7f9"
     ]
    },
    "id": "joHdhjyc3_y2",
    "outputId": "4688a4ac-8801-480a-9e2b-be447fa85131"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=50,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")\n",
    "\n",
    "def format_instruction(example):\n",
    "    return f\"<s>[INST] {example['question']} [/INST]\\n{example['answer']}</s>\"\n",
    "\n",
    "def tokenize_function(example):\n",
    "    text = format_instruction(example)\n",
    "    return tokenizer(text, truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=training_arguments,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "print(\"Starting the QLoRA fine-tuning process...\")\n",
    "trainer.train()\n",
    "print(\"Training complete.\")\n",
    "\n",
    "import shutil\n",
    "\n",
    "print(\"Saving the trained LoRA adapters...\")\n",
    "trainer.model.save_pretrained(NEW_ADAPTERS_ID)\n",
    "\n",
    "shutil.make_archive(\"algo-tutor-lora-adapters\", 'zip', NEW_ADAPTERS_ID)\n",
    "print(\"Adapters saved and zipped to 'algo-tutor-lora-adapters.zip'.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
